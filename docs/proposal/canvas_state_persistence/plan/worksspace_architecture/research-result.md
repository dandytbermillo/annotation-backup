Plan vs. Reality Audit (Phases 1–3)
Phase 1 – Versioned Workspace Metadata
Database Schema (version field) – Missing. The plan calls for adding a version INT column to canvas_workspace_notes (and possibly panels), but no such column or migration is present in the code (the provided placeholder_add_canvas_workspace_version.md is a stub with instructions). No API or query references a workspace version, indicating this schema change hasn’t been implemented.
Transactional Panel Close & Version Increment – Not implemented. The intended approach was to wrap panel close in one DB transaction that closes the panel and bumps the workspace version. In reality, the close operation is split: the client calls the workspace API to mark the note closed, then calls the layout API to update the panel state separately. There’s no unified SQL transaction across tables, and notably no version = version + 1 update on close. The canvas_workspace_notes table is updated without touching a version field (since none exists), and the panel’s state is updated in isolation (with just a revision token increment).
Cache Invalidation on Close – Missing. According to Phase 1, closing a panel should immediately invalidate any cached snapshot (localStorage.note-data-$id). The current code does not remove the local cache on close – e.g. in closeNote() there is no call to clear stored panel layout data. This means a stale cache might linger after a panel is closed, contrary to the plan.
Reopen Handling – Not implemented. The plan suggests that reopening a note should increment the version and refresh the snapshot with new server state. In code, reopening (openNote()) simply adds the note back to state and persists its open status; it does not interact with a version counter at all. The snapshot saved to cache on reopen contains panel positions but no version tag.
GET /api/canvas/workspace Response – No version included. Phase 1 required the workspace fetch API to return the current version. The implemented GET /api/canvas/workspace returns open notes and panels, but no version field is present. The WorkspaceNote and PanelSnapshot interfaces in the route have no version attribute. This is a gap – clients can’t know the server’s version of a workspace.
Cache-vs-Server Hydration Logic – Not implemented. The app does not compare a cached snapshot’s version to the server’s. Instead, on load it always tries to fetch the latest panels from the server (with a 10s timeout), then falls back to local cache only if the fetch fails. There’s no check like “if snapshot.version === server.version” as proposed. Also, the code doesn’t memoize any version per note – since no version is tracked, it re-fetches on each load regardless.
Phase 2 – Cache Simplification
Snapshot Data Scope – Partially implemented. The current local cache for canvas state is already minimal, storing primarily panel layout and camera state with timestamps. For example, saveToCacheWithExpiry() writes { data: panels, timestamp } to localStorage. No extraneous data (like full note content or branch data) is saved. However, the planned structure { version, panels, savedAt } is not fully in use – the cached item includes panels and a timestamp (equivalent to savedAt), but no version field is stored or checked.
Global TTL Enforcement – Implemented, but longer than proposed. The code uses a TTL for cached data, but it’s set to 7 days. Any cached panel layout older than this is treated as expired and not used (the code removes it). This aligns with the concept of auto-expiring caches, though the plan suggested a 24-hour TTL as an example. Currently, the TTL is significantly larger, so in practice a “stale” cache could live for up to a week. Adjusting this duration down to 24 hours may be needed per the plan.
IndexedDB Offline Queue – No version awareness. The canvasOfflineQueue system is intact and handles failed panel updates by queueing them in IndexedDB. It periodically flushes when back online. What’s missing is any integration of workspace version into these operations. Queued CanvasOperation entries do not record a workspace version, only a timestamp and type/data payload. On flush, the client does not check if the server’s version has advanced before replaying changes – it simply replays the recorded PATCH requests. This could lead to stale writes: e.g. an offline queued panel change might apply to a workspace that has since changed on another device. The plan calls for including version in the payload or at least validating the version on replay, which is not yet done. (Conflict handling currently relies only on per-panel revisionToken checks in the PATCH logic, which is narrower in scope than a full workspace version handshake.)
Phase 3 – Automated Reconciliation (No Prompt)
Automated Hydration w/ Version Check – Not in place. The desired flow was to fetch server state (panels plus version) and use the local snapshot only if versions match. Right now, the hydration uses a simpler logic: always try server first, then use the cache if the server call times out or fails. This does achieve automatic reconciliation in most online cases (server data will override any local state without user intervention). But because no version is compared, the app can’t detect a subtle stale cache if the server fetch succeeds – it just blindly overrides the cache. In offline cases, it will use whatever cache exists (since it has no version to warn it), potentially resurrecting ghost panels if the cache was outdated. In summary, the spirit of “no user prompt, prefer server truth” is present, but the safety net of version matching is absent.
Banner/Prompt Removal – Achieved. The earlier plan for a “ghost panel confirmation” banner has been abandoned, and accordingly no such UI prompt exists in the code. The Canvas provider/hydration flow does not ask the user to resolve conflicts – it handles them programmatically. This aligns with Phase 3’s goal of silent reconciliation.
Telemetry for Cache Usage – Not implemented. Phase 3 called for logging events like canvas.cache_used, canvas.cache_mismatch, canvas.cache_discarded to monitor how often local caches are used or invalidated. The current code does produce debug logs for related situations (e.g. logging when it falls back to cached panels or when it skips using a cache). However, these are internal logs; they are not structured telemetry events visible to a monitoring system. There’s no implementation of the specific counters or analytics mentioned in the plan.
Testing “Ghost Panel” Scenarios – Gap. An integration test was expected to ensure that closing a panel increments the version and that a stale snapshot is ignored on reload. Since versioning isn’t there, such a test would currently fail. We did not find new automated tests targeting the ghost panel issue specifically. For example, a scenario where a panel is closed and then the app is reloaded offline (forcing use of an old cache) could still result in the panel reappearing – a clear indication that without version checks, the ghost panel problem isn’t entirely solved. Additional test coverage and possibly telemetry (to catch if ghost panels ever “resurrect”) will be needed once versioning is in place.
Validation Matrix & Scenario Outcomes
Below is a set of key scenarios (derived from the hardening plan’s goals and deliverables) with proposed test steps. For each, we note whether the current implementation would pass or fail the scenario and why:
Scenario 1: Panel Close Persistence – “Closing a panel increases version and no ghost on reload.”
Test: Open a note in the canvas (ensure it’s tracked as open). Close its main panel. Then reload the app and fetch the workspace state.
Expected: The workspace’s version is incremented by the close action, and the closed panel does not return after reload. Any cached snapshot with the old version is discarded.
Current: Fail (partial). Closing a panel does mark it closed in the backend (isOpen=false, panel state ‘closed’), but since no version is recorded, the app can’t verify on reload whether a cache is stale. If the reload is online, the server correctly omits the closed panel (so no ghost panel – this appears to pass). However, if the reload occurs offline (using a leftover cache), the closed panel would reappear from localStorage, because the cache wasn’t invalidated and there’s no version check to prevent it. In sum, the ghost panel is eliminated in typical online flows (by always favoring server data), but the underlying version mechanism to guarantee it in all cases is not in place – a test that forces a stale cache usage would reveal the issue.
Scenario 2: Concurrent Open/Close on Different Devices – “No ghost panel when workspace changes elsewhere.”
Test: On Device A, close a note’s panel (thus in a versioned world, version++). Without refreshing Device B, open the same workspace on Device B (which still has a cached state where that panel was open).
Expected: Device B detects the server’s higher version and ignores its outdated cache, so the panel stays closed (no “ghost” reopen). User is not prompted – the reconciliation is automatic.
Current: Likely pass (with caveat). When Device B loads, it will call GET /api/canvas/workspace. Today, that response doesn’t include a version, but it will accurately reflect that the note is closed (since Device A updated the DB). Device B will then fetch the panel layout and see no active panel. It will overwrite any local cache with the server state, so the panel remains closed. In effect, Device B won’t resurrect the panel (no ghost) because the implementation always favors fresh server data on load. This scenario passes in practice. (However, this is happening implicitly; if the API failed and Device B fell back to a stale cache, the outcome would differ. Once proper version fields exist, this scenario can be explicitly validated via version mismatch logic rather than by assumption of network success.)
Scenario 3: Stale Cache vs. Server Truth – “Cached snapshot is used only if still valid.”
Test: Simulate a user returning to the app after a period of inactivity. They have a local cached workspace snapshot (panels layout) from earlier. In the meantime, the server state changed (panels moved/closed, etc.). On app launch, allow it to load using either cache or server.
Expected: If the local snapshot’s savedAt or version doesn’t match the server’s current state, the app should discard the cache and use server data, logging a cache mismatch event. Only if the cache is up-to-date (same version) should it hydrate from local data.
Current: Mostly pass (online) / fail (offline). The app never blindly trusts a cache when online – it always fetches the latest open notes and panel layouts. In an online launch, the stale cache will simply get overwritten by fresh server panels (the code even logs that it loaded panels from the server). In that sense, it “discards” the stale snapshot by ignoring it – passing the intent of this test (though without explicitly checking a version number). There is no user prompt; the reconciliation is silent. However, if the user is offline or the server call fails, the app will use the cached snapshot (regardless of how old or stale it is). In that case, since no version was compared, a very stale cache could be accepted, and outdated panels would appear. So the scenario of a true version mismatch isn’t fully handled in offline conditions. This is a known gap that versioning will address.
Scenario 4: Cache Expiry (TTL) – “Old local caches expire after TTL.”
Test: Start the app after not using it for longer than the cache TTL (e.g. 2 days later, assuming a 24h TTL). Also test within the TTL (e.g. 2 hours later).
Expected: If the cache is older than the TTL (24h in plan), it should be ignored or deleted, forcing a fresh server load. If within TTL and no version change, the cache might be used (if versions match).
Current: Pass (with current TTL settings). The implementation tags each cache entry with a timestamp and checks age on load. If the cached item is older than the defined TTL (currently 7 days), the code treats it as expired and will not use it – it removes the item and proceeds as if no cache exists. In our test, a 2-day-old cache is still under 7 days and would be considered valid (whereas the plan’s stricter 24h policy would have invalidated it). A cache older than 7 days is correctly dropped. So, the mechanism works, but the threshold is more lenient than the plan envisions. Once the TTL is adjusted to 24 hours, a next-day launch would indeed bypass the cache (which is the goal). Currently, a next-day (24–48h) old cache would still be used, which is a slight policy mismatch. (This is a minor gap – configuration rather than a functional bug.)
Scenario 5: Offline Edits vs. Server Changes – “Offline queue doesn’t overwrite newer server state.”
Test: User goes offline and makes changes – e.g. drags a panel or closes a note (these operations queue in IndexedDB). Before the app comes back online, another device or user changes the same workspace (panels moved or note closed/opened, increasing the server’s version). Then bring the first app online and let it flush the offline queue.
Expected: The app should detect that its queued operations are against an outdated version. Ideally, it would refuse to apply them or merge intelligently to avoid inconsistencies. No ghost panels should revive; no out-of-sync panel positions should overwrite newer ones. This might be tracked via a version check on flush (or each queued item carrying the last known version and the server rejecting mismatched version updates).
Current: Potential fail. In the current setup, offline queued operations do not check any workspace version before applying. When back online, the app will attempt to replay the queued REST calls exactly as recorded. The only conflict detection in place is at the panel level – e.g. the PATCH for panel updates includes an expectedRevision (revision token) which could fail if the panel was concurrently modified. This means some conflicts (like two edits to the same panel position) might be caught and not applied. However, higher-level conflicts, such as the entire note having been closed or reopened elsewhere, are not recognized via a version field. For example, if the user’s offline queue contains “open note X” but the note was actually closed (version bumped) on the server, the offline flush will still send the open request. The server will happily mark it open again since there’s no version safeguard (it just upserts is_open=true) – effectively resurrecting a panel that was supposed to stay closed. This scenario would result in a ghost panel or layout divergence, indicating a failure to protect against stale operations. Only once versioning is implemented (and perhaps the server rejects updates with an old version) will this test reliably pass.
Scenario 6: Telemetry & Monitoring – “Detect frequent cache mismatches or failures.”
Test: Induce a cache mismatch (e.g. stale cache vs updated server) and a cache use (e.g. offline use of cache). Check that appropriate telemetry events (canvas.cache_mismatch, canvas.cache_used, etc.) are logged, and that counters/alerts (if any) would register this.
Expected: The system should emit structured events when it discards a snapshot or when it uses one, as outlined in Phase 3. Over time, product ops could monitor these to gauge how often clients are out-of-sync (repeated mismatches might warrant investigation).
Current: Fail. There are no telemetry events for this in the code yet. In the test, although the app would handle the mismatch internally, we would only see developer console logs or debug logs (e.g. debugLog({ action: 'using_cached_panels' }) when it falls back to cache). These are not exposed as analytics events to any dashboard. As a result, Product/Engineering cannot currently quantify “version drift” or cache issues in the field – a gap in monitoring. Implementing those log events and perhaps adding server-side tracking (as mentioned in Phase 5) will be necessary for full validation.
Additional Gaps: Beyond the scenarios, our review noted that automated tests corresponding to these situations are lacking. The plan’s deliverables include unit tests for version bumps and an integration test for cache invalidation – those aren’t present, meaning these behaviors aren’t being continuously verified. We also haven’t seen updates to documentation or runbooks regarding the new version field (e.g. how to reset a version or handle migrations), which will be important when Phase 1 is implemented. Logging and telemetry around version mismatches (to feed into Phase 5’s monitoring) remain to be added as well. In summary, the groundwork in code for Phases 1–3 is only partially there – significant new work (especially implementing the version field end-to-end and adjusting caching logic) will be required to meet the plan’s goals fully.
Sources: The analysis above is based on the Ghost Panel Remedy plan and the current code in files like canvas-workspace-context.tsx, use-canvas-hydration.ts, use-panel-persistence.ts, canvas-offline-queue.ts, and the canvas API routes, which were inspected to verify actual behaviors.